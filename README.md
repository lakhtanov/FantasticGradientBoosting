# FantasticGradientBoosting
# Авторы: Иван Лахтанов (lakhtanov) и Роман Александров (rialeksandrov)

В данном библиотеке реализована утилита, позволяющая обучать градиентный бустинг на решающих деревьях для задач классификации, а также применять обученный градиентный бустинг на тестовых данных.

### Сборка проекта
Для сборки проекта необходимо:
1. Склонировать репозиторий с проектом:

`git clone https://github.com/lakhtanov/FantasticGradientBoosting`

2. Собрать утилиту при помощи `cmake`:

`cmake CMakeLists.txt`

В результате этих действий в папке с репозиторием появится исполняемый файл `FantasticGradientBoosting`, являющийся утилитой, использующейся для обучения/применения модели.

### Интерфейс утилиты
У утилиты `FantasticGradientBoosting` есть два режима: обучение и применение.

#### Обучение
Для обучения модели необходимо запустить утилиту `FantasticGradientBoosting`, передав ей в качестве параметра `config=train.conf` конфигурационный файл `train.conf`, описывающий параметры тренируемой модели (такие как: тип тренируемых деревьев, количество тренируемых деревьев, глубина тренируемых деревьев, learning rate, количество ядер процессора, участвующих в обучении и др.). Также соответствующие параметры можно передавать, используя параметры командной строки, при этом в этом случае они будут иметь больший приоритет. Например:

`./FantasticGradientBoosting config=train.conf num_trees=10`

запустит обучение модели с параметрами, описанными в конфигурационном файле `train.conf`, при этом количество деревьев тренируемой модели будет равно `10`, так как соответствующий параметр был передан через параметры командной строки и, соответственно, имеет больший приоритет по сравнению с параметрами, описанными в конфигурационном файле.

##### Параметры обучения модели
`tree_type` - тип деревьев, используемых в обучаемой модели (вариант по умолчанию `tree_type=GradientBoostingTreeOblivious`)

`num_trees` - количество деревьев в обучаемой модели (вариант по умолчанию `num_trees=10`)

`depth` - глубина тренируемых деревьев (вариант по умолчанию `depth=6`)

`learning_rate` - learning rate модели (вариант по умолчанию `learning_rate=1e-2`)

`num_cores` - количество ядер, используемых для обучения модели (вариант по умолчанию `num_cores=-1` - использовать все доступные ядра процессора)

`loss_funciton` - оптимизируемая функция потерь (вариант по умолчанию `loss_function=GradientBoostingMSELossFunction` - квадратичная функция потерь)

`binarization_method` - метод бинаризации признаков, используемый при обучении/применении объектов; данный параметр может встречаться в конфиге несколько раз с разными значениями - тогда бинаризация признаков будет производиться каждым из указанных методов (варианты по умолчанию `binarization_method=BinCreatorByAbsoluteValue;10,20,30,40` и `binarization_method=BinCreatorByStatistics;50,60,70,80`, где после точки с запятой указаны параметры для каждого из методов бинаризации)

`train_data_percentage` - процент данных обучающей выборки, на которых производится предварительное обучение каждого следующего обучающего дерева (вариант по умолчанию `train_data_percentage=100`; для более подробного описания того, на что влияет этот параметр, смотрите параграф `Архитектура проекта`)

`train_features_percentage` - процент признаков обучающей выборки, на которых производится предварительное обучение каждого следующего обучающего дерева (вариант по умолчанию `train_features_percentage=100`; для более подробного описания того, на что влияет этот параметр, смотрите параграф `Архитектура проекта`)

`num_possible_train_trees` - количество деревьев, построенных в качестве потенциального следующего обучающего дерева(вариант по умолчанию `num_possible_train_trees=1`, обозначающий, что на каждой итерации строится только одно наилучшее обученное дерево; для более подробного описания того, на что влияет этот параметр, смотрите параграф `Архитектура проекта`)

`input_file` - путь к файлу в формате `.csv`, в котором расположены обучающие данные (вариант по умолчанию `input.csv`)

#### Применение
Перед запуском применения модели обязательно необходимо провести процедуру обучения модели, иначе модель запускаться не будет. Для применения модели необходимо запустить утилиту `FantasticGradientBoosting`, передав ей в качестве параметра `config=test.conf` конфигурационный файл `test.conf`, описывающий параметры, использующиеся при предсказании модели. Также соответствующие параметры можно передавать, используя параметры командной строки, при этом в этом случае они будут иметь больший приоритет. Например:

`./FantasticGradientBoosting config=test.conf output_file=output.csv`

запустит применение модели с параметрами, описанными в конфигурационном файле `test.conf`, при этом результаты применения модели будут записаны в файл `output.csv`, так как соответствующий параметр был передан через параметры командной строки и, соответственно, имеет больший приоритет по сравнению с параметрами, описанными в конфигурационном файле.

##### Параметры применения модели

`num_cores` - количество ядер, используемых при применении модели (вариант по умолчанию `num_cores=-1` - использовать все доступные ядра процессора)

`output_file` - путь к файлу в формате `.csv`, в который будут выведены результаты предсказания модели (вариант по умолчанию `output.csv`)

### Архитектура проекта
Интерфейсом для обучения модели является класс `GradientBoosting`, имеющий методы `GradientBoosting::fit()` и `GradientBoosting::predict()`, отвечающие за обучение и применение обученной модели, соответственно. Настройка параметров обучения/применения градиентного бустинга производится путем передачи в соответсвующие методы класса `GradientBoosting` объекта типа `GradientBoostingConfig`, содержащего внутри себя всю информацию.

#### Обучение модели
Обучение модели (осуществляющееся в результате вызова метода `GradientBoosting::fit()`) происходит в несколько этапов:

0. Все объекты обучающей выборки случайным образом перемешиваются.
1. Для признаков данных из обучающей выборки проводится бинаризация всеми методами, указанными при конфигурации.
2. Полученная бинаризованная обучающая выборка выкладывается в памяти поколоночно, то есть сначала в памяти идут значения первого признака для всех объектов, потом второго, потом третьего и так далее. Это сделано для того, чтобы ускорить работу алгоритма обучения - обычно при обучении деревьев в градиентном бустинге объекты производится много обращений к одному и тому же признаку для разных объектов обучающей выборки - соответственно, чтобы соответствующие обращения к памяти как можно чаще попадали в кэш, выгодно выложить данные в памяти так, чтобы значения признака для каждого конкретного признака и для всех объектов шли в памяти подряд.
3. Каждое последующее дерево в градиентном бустинге строится следующим образом: параллельно строятся `num_possible_train_trees` обучающих деревьев, причем для каждого в качестве обучающей подвыборки выбирается некоторый случайный непрерывный по объектам отрезок данных (то есть такой, что для каждого признака значения соответствующего признака для выбранных объектов лежат в памяти подряд), причем процент количества выбранных данных относительно всего количества данных в обучающей выборке равен `train_data_percentage`. При построении соответствующих деревьев также используются лишь `train_features_percentage` процент признаков от всего количества признаков, причем соответствующие признаки выбираются случайным образом. Таким образом, будут построены `num_possible_train_trees` обучающих деревьев. Для каждого из них параллельно выясняется, насколько хорошо оно приближает соответствующий градиент для обучающей выборки и для соответствующей функции потерь `loss_function`, и выбирается дерево, показавшее наилучший результат.
4. Выбранное на шаге `3` обучающее дерево добавляется в ансамбль уже обученных решающих деревьев с весом `learning_rate`.

#### Применение модели
Применение модели (осуществляющееся в результате вызова метода `GradientBoosting::predict()`) происходит в несколько этапов:

1. 

### Описание экспериментов
Сравнение скорости работы и качества данной утилиты производилось с популярными аналогами *xgboost* и *lightgbm*. Были проведены эксперименты на двух датасетах

#### Бенчмарки на датасете [Higgs](https://www.kaggle.com/c/higgs-boson/data)

###### Выводы

#### Бенчмарки на датасете [BCI](https://www.kaggle.com/c/inria-bci-challenge#evaluation)

###### Выводы
